services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    volumes:
      - ~/.cache/huggingface:/models
    ports:
      - 33831:8080
    command: >
      --server
      --hf-repo bartowski/Tess-v2.5-Phi-3-medium-128k-14B-GGUF --hf-file Tess-v2.5-Phi-3-medium-128k-14B-Q6_K.gguf --model /models/Tess-v2.5-Phi-3-medium-128k-14B-Q6_K.gguf
      --port 8080
      --host 0.0.0.0