services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    container_name: llamacpp
    volumes:
      - ~/.cache/huggingface:/models
    ports:
      - 33831:8080
    command: >
      --server
      --model ${HARBOR_LLAMACPP_MODEL}
      --port 8080
      --host 0.0.0.0
    networks:
      - harbor-network

  webui:
    environment:
      - ENABLE_OPENAI_API=true