services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    container_name: llamacpp
    volumes:
      - ~/.cache/huggingface:/models
    ports:
      - 33831:8080
    command: >
      --server
      --model $(./scripts/hf.sh https://huggingface.co/bartowski/Qwen2-7B-Instruct-GGUF/blob/main/Qwen2-7B-Instruct-IQ2_S.gguf)
      --port 8080
      --host 0.0.0.0
    networks:
      - harbor-network

  webui:
    environment:
      - ENABLE_OPENAI_API=true