Answer with a single JSON object to the question about Harbor CLI.

Harbor CLI is quite extensive, make sure to not make mistakes. Suggest additional commands that will help the User validating their actions. Certain questions might require multiple commands to run, that's ok and expected. Put them into "setupCommands" array.

Harbor is a Containerized LLM toolkit. It allows the User to run LLM backends, APIs, frontends, and additional services via a concise CLI. Services and containers are the same things. Harbor works on top of Linux Shell and Docker Compose.

Here's CLI help:

Usage: harbor <command> [options]

Compose Setup Commands:
  harbor up|u         - Start the containers
  harbor down|d       - Stop and remove the containers
  harbor restart|r    - Down then up
  harbor ps           - List the running containers
  harbor logs|l       - View the logs of the containers
  harbor exec         - Execute a command in a running service
  harbor pull         - Pull the latest images
  harbor dive         - Run the Dive CLI to inspect Docker images
  harbor run          - Run a one-off command in a service container
  harbor shell        - Load shell in the given service main container
  harbor build        - Build the given service
  harbor cmd <handle> - Print the docker-compose command

Setup Management Commands:
  harbor ollama     - Run Ollama CLI (docker). Service should be running.
  harbor smi        - Show NVIDIA GPU information
  harbor top        - Run nvtop to monitor GPU usage
  harbor llamacpp   - Configure llamacpp service
  harbor tgi        - Configure text-generation-inference service
  harbor litellm    - Configure LiteLLM service
  harbor openai     - Configure OpenAI API keys and URLs
  harbor vllm       - Configure VLLM service
  harbor aphrodite  - Configure Aphrodite service
  harbor tabbyapi   - Configure TabbyAPI service
  harbor mistralrs  - Configure mistral.rs service
  harbor cfd        - Run cloudflared CLI

Service CLIs:
  harbor parllama          - Launch Parllama - TUI for chatting with Ollama models
  harbor plandex           - Launch Plandex CLI
  harbor interpreter|opint - Launch Open Interpreter CLI
  harbor hf                - Run the Harbor's Huggingface CLI. Expanded with a few additional commands.
  harbor hf dl           - HuggingFaceModelDownloader CLI
  harbor hf parse-url    - Parse file URL from Hugging Face
  harbor hf token        - Get/set the Hugging Face Hub token
  harbor hf find <query> - Open HF Hub with a query (trending by default)
  harbor hf *            - Anything else is passed to the official Huggingface CLI

Harbor CLI Commands:
  harbor open handle                   - Open a service in the default browser

  harbor url <handle>                  - Get the URL for a service
  harbor url <handle>                         - Url on the local host
  harbor url [-a|--adressable|--lan] <handle> - (supposed) LAN URL
  harbor url [-i|--internal] <handle>         - URL within Harbor's docker network

  harbor qr <handle>            - Print a QR code for a service

  harbor t|tunnel <handle>             - Expose given service to the internet
  harbor tunnel down|stop|d|s        - Stop all running tunnels (including auto)
  harbor tunnels [ls|rm|add]         - Manage services that will have tunnels by default on "harbor up"
  harbor tunnels rm <handle|index>   - Remove, also accepts handle or index
  harbor tunnels add <handle>        - Add a service to the tunnel list

  harbor config [get|set|ls]         - Manage the Harbor environment configuration
  harbor config ls                   - All config values in ENV format
  harbor config get <field>          - Get a specific config value
  harbor config set <field> <value>  - Get a specific config value
  harbor config reset                - Reset Harbor configuration to default.env

  harbor defaults [ls|rm|add]        - List default services
  harbor defaults rm <handle|index>  - Remove, also accepts handle or index
  harbor defaults add <handle>       - Add

  harbor ls|list [--active|-a]       - List available/active Harbor services
  harbor ln|link [--short]           - Create a symlink to the CLI, --short for 'h' link
  harbor unlink                      - Remove CLI symlinks
  harbor eject                       - Eject the Compose configuration, accepts same options as 'up'
  harbor help|--help|-h              - Show this help message
  harbor version|--version|-v        - Show the CLI version
  harbor gum                         - Run the Gum terminal commands
  harbor fixfs                       - Fix file system ACLs for service volumes
  harbor info                        - Show system information for debug/issues

```bash
# to enable searxng for WebRAG in webui?
harbor up searxng

# to Run additional/alternative LLM Inference backends. Open Webui is automatically connected to them.
harbor up llamacpp tgi litellm vllm tabbyapi aphrodite

# to setup service models
harbor tgi model google/gemma-2-2b-it
harbor vllm model google/gemma-2-2b-it
harbor aphrodite model google/gemma-2-2b-it
harbor tabbyapi model google/gemma-2-2b-it-exl2
harbor mistralrs model google/gemma-2-2b-it
harbor opint model google/gemma-2-2b-it

# Run different Frontends
harbor up librechat bionicgpt hollama

# Stop a single service
harbor stop searxng

# Set webui version
harbor webui version 0.3.11

# Use custom models for supported backends
harbor llamacpp model https://huggingface.co/user/repo/model.gguf

# Open HF Hub to find the models
harbor hf find gguf gemma-2

# Use HFDownloader and official HF CLI to download models
harbor hf dl -m google/gemma-2-2b-it -c 10 -s ./hf
harbor hf download google/gemma-2-2b-it

# Show LAN URL for vllm
harbor url -a vllm

# Pass down options to docker-compose
harbor down --remove-orphans

# Restart a single specific service only
harbor restart tabbyapi

# Pull the latest images for additional services
harbor pull searxng

# Build a service with a dockerfile
harbor build hfdownload

# Show logs for a specific service
harbor logs webui

# Update all images
harbor pull

# Show last 200 lines of logs
harbor logs webui -n 200

# Check the processes in ollama container
 harbor exec ollama ps aux

# Ping one service from the other one?
harbor exec webui curl $(harbor url -i ollama)

# Generate a QR code in terminal?
harbor run qrgen http://example.com

# Run docker compose with harbor files on my own?
$(harbor cmd "webui") <your command>

# Launch interactive shell to test the container?
harbor shell mistralrs
```

Respond in JSON with the following schema:
{
  "setupCommands": [],
  "desiredCommand": "string",
  "nonInteractive": "yes|no",
  "safetyLevel": "delete|overwrite|safe",
  "assistantMessage": "string"
}

Example 1:
"to generate a QR code in terminal?"
{
  "setupCommands": [],
  "desiredCommand": "harbor run qrgen http://example.com",
  "nonInteractive": "yes",
  "safetyLevel": "safe",
  "assistantMessage": "One of the cool features! You can generate QR codes for terminal for arbitrary URLs with Harbor."
}

Example 2:
"to see logs of one specific service?"
{
  "setupCommands": ["harbor up webui"],
  "desiredCommand": "harbor logs webui",
  "nonInteractive": "yes",
  "safetyLevel": "safe",
  "assistantMessage": "This command will show last few lines of logs of webui and then will start tailing new logs. Can be combined with grep."
}

Example 3:
"to stop all running containers?"
{
  "setupCommands": [],
  "desiredCommand": "harbor down",
  "nonInteractive": "yes",
  "safetyLevel": "safe",
  "assistantMessage": "This command will stop all the Harbor services that are currently running"
}
