# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░
# ▒█░▒█ ░█▀▀█ ▒█▀▀█ ▒█▀▀█ ▒█▀▀▀█ ▒█▀▀█
# ▒█▀▀█ ▒█▄▄█ ▒█▄▄▀ ▒█▀▀▄ ▒█░░▒█ ▒█▄▄▀
# ▒█░▒█ ▒█░▒█ ▒█░▒█ ▒█▄▄█ ▒█▄▄▄█ ▒█░▒█
# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

# Harbor Configuration.
# ---------------------
# This section contains environment variables that are
# used in Harbor's compose files, configs and can be modified via harbor CLI.
#
# | Using CLI for config management:
# |
# | ```bash
# | harbor config get hf.cache
# | harbor config set hf.cache ~/.cache/huggingface
# | ```
# |
# | See more at https://github.com/av/harbor/wiki/Harbor-CLI-Reference

# Abstract/shared
# ---------------------

HARBOR_HF_CACHE="~/.cache/huggingface"
HARBOR_HF_TOKEN=""

HARBOR_LLAMACPP_CACHE="~/.cache/llama.cpp"
HARBOR_OLLAMA_CACHE="~/.ollama"

# These could be used by specific services,
# in which case they can be set in a centralised
# location like this.
HARBOR_ANYSCALE_KEY=""
HARBOR_APIPIE_KEY=""
HARBOR_COHERE_KEY=""
HARBOR_FIREWORKS_API_KEY=""
HARBOR_GROQ_KEY=""
HARBOR_MISTRAL_KEY=""
HARBOR_OPENROUTER_KEY=""
HARBOR_PERPLEXITY_KEY=""
HARBOR_SHUTTLEAI_KEY=""
HARBOR_TOGETHERAI_KEY=""
HARBOR_ANTHROPIC_KEY=""
HARBOR_BINGAI_TOKEN=""
HARBOR_GOOGLE_KEY=""
HARBOR_ASSISTANTS_KEY=""

HARBOR_UI_MAIN="webui"
HARBOR_UI_AUTOOPEN=false
HARBOR_SERVICES_DEFAULT="ollama;webui"
HARBOR_SERVICES_TUNNELS=""
HARBOR_CONTAINER_PREFIX="harbor"
HARBOR_CLI_NAME="harbor"
HARBOR_CLI_SHORT="h"
HARBOR_CLI_PATH="~/.local/bin"

# OpenAI
# ---------------------
# In the Context of Harbor, it means OpenAI API-compatible
# services, such as Ollama, Llama.cpp, LiteLLM, etc.

HARBOR_OPENAI_URLS=""
HARBOR_OPENAI_KEYS=""

# This variable is derived as a first item in the list above
HARBOR_OPENAI_KEY=""
HARBOR_OPENAI_URL=""

# webui
HARBOR_WEBUI_HOST_PORT=33801
# Persistent secret - user stays logged into
# webui between restarts
HARBOR_WEBUI_SECRET="h@rb0r"
HARBOR_WEBUI_NAME="Harbor"
HARBOR_WEBUI_LOG_LEVEL="INFO"
HARBOR_WEBUI_VERSION="main"

# llamacpp
HARBOR_LLAMACPP_HOST_PORT=33831
HARBOR_LLAMACPP_MODEL="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-q4.gguf"
HARBOR_LLAMACPP_MODEL_SPECIFIER="--hf-repo microsoft/Phi-3-mini-4k-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf"
HARBOR_LLAMACPP_EXTRA_ARGS="-ngl 32"

# ollama
HARBOR_OLLAMA_HOST_PORT=33821

# litellm
HARBOR_LITELLM_HOST_PORT=33841
HARBOR_LITELLM_MASTER_KEY="sk-litellm"
HARBOR_LITELLM_DB_HOST_PORT=33842
HARBOR_LITELLM_UI_USERNAME="admin"
HARBOR_LITELLM_UI_PASSWORD="admin"

# lmdeploy
HARBOR_LMDEPLOY_HOST_PORT=33831

# searxng
HARBOR_SEARXNG_HOST_PORT=33811

# tgi (text-generation-inference)
HARBOR_TGI_HOST_PORT=33851
HARBOR_TGI_MODEL="google/gemma-2-2b-it"
HARBOR_TGI_QUANT=""
HARBOR_TGI_REVISION=""
HARBOR_TGI_EXTRA_ARGS="--max-concurrent-requests 16"
HARBOR_TGI_MODEL_SPECIFIER="--model-id google/gemma-2-2b-it"

# tts (openedai-sppech)
HARBOR_TTS_HOST_PORT=33861
HARBOR_TTS_HOME="voices"
HARBOR_TTS_VOICES_FOLDER="./tts/voices"
HARBOR_TTS_CONFIG_FOLDER="./tts/config"

# hollama
HARBOR_HOLLAMA_HOST_PORT=33871

# LangFuse
HARBOR_LANGFUSE_HOST_PORT=33881
HARBOR_LANGFUSE_NEXTAUTH_SECRET="langfuse"
HARBOR_LANGFUSE_SALT="salt"
HARBOR_LANGFUSE_DB_HOST_PORT=33882
# These should be set when configuring
# new project in the service
HARBOR_LANGFUSE_PUBLIC_KEY=""
HARBOR_LANGFUSE_SECRET_KEY=""

# LibreChat
HARBOR_LIBRECHAT_HOST_PORT=33891
HARBOR_LIBRECHAT_RAG_HOST_PORT=33892

# BionicGPT
HARBOR_BIONICGPT_HOST_PORT=33901

# vLLM
HARBOR_VLLM_HOST_PORT=33911
HARBOR_VLLM_MODEL="microsoft/Phi-3-mini-4k-instruct"
HARBOR_VLLM_EXTRA_ARGS=""
HARBOR_VLLM_ATTENTION_BACKEND="FLASH_ATTN"
HARBOR_VLLM_MODEL_SPECIFIER="--model microsoft/Phi-3-mini-4k-instruct"

# Aphrodite
HARBOR_APHRODITE_HOST_PORT=33921
HARBOR_APHRODITE_EXTRA_ARGS=""
HARBOR_APHRODITE_MODEL="neuralmagic/Mistral-7B-Instruct-v0.3-GPTQ-4bit"

# TabbyAPI
HARBOR_TABBYAPI_HOST_PORT=33931
HARBOR_TABBYAPI_ADMIN_KEY="adk-tabbyapi"
HARBOR_TABBYAPI_API_KEY="apk-tabbyapi"
HARBOR_TABBYAPI_MODEL="Annuvin/gemma-2-2b-it-abliterated-4.0bpw-exl2"
HARBOR_TABBYAPI_MODEL_SPECIFIER="Annuvin_gemma-2-2b-it-abliterated-4.0bpw-exl2"
HARBOR_TABBYAPI_EXTRA_ARGS=""

# Parllama
HARBOR_PARLLAMA_CACHE="~/.parllama"

# Plandex
HARBOR_PLANDEX_HOST_PORT=33941
HARBOR_PLANDEX_DB_HOST_PORT=33942
HARBOR_PLANDEX_HOME="~/.plandex-home"

# Mistral.rs
HARBOR_MISTRALRS_HOST_PORT=33951
HARBOR_MISTRALRS_MODEL_TYPE="plain"
HARBOR_MISTRALRS_MODEL="microsoft/Phi-3-mini-4k-instruct"
HARBOR_MISTRALRS_MODEL_ARCH="phi3"
HARBOR_MISTRALRS_MODEL_ISQ=""
HARBOR_MISTRALRS_MODEL_SPECIFIER="plain -m microsoft/Phi-3-mini-4k-instruct -a phi3"
HARBOR_MISTRALRS_EXTRA_ARGS=""

# Open Interpreter
HARBOR_OPINT_CONFIG_PATH="~/.config/open-interpreter"
HARBOR_OPINT_EXTRA_ARGS=""
HARBOR_OPINT_MODEL="llama3.1"
HARBOR_OPINT_CMD="--model llama3.1"
HARBOR_OPINT_BACKEND=""

# cmdh
HARBOR_CMDH_MODEL="llama3.1"

# ============================================
# Service Configuration.
# You can specify any of the service's own environment variables here.
# ============================================

# Open WebUI
# See https://docs.openwebui.com/getting-started/env-configuration/ for reference.
# --------------------------------------------
# WEBUI_NAME=WUI

# Ollama Configuration.
# Run harbor ollama serve --help for a list of env vars
# --------------------------------------------
# OLLAMA_DEBUG=1